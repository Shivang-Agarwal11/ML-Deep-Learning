{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2,preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shiva\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\PIL\\Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY = r\"dataset/\"\n",
    "CATEGORIES = [\"with_mask\", \"without_mask\"]\n",
    "training_data = []\n",
    "labels = []\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY, category)\n",
    "    indexes=CATEGORIES.index(category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = load_img(img_path, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "        image=preprocess_input(image)\n",
    "        training_data.append(image)\n",
    "        labels.append(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=np.array(training_data)\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(training_data, labels,test_size=0.20, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = ImageDataGenerator(rotation_range=20,zoom_range=0.15,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.15,horizontal_flip=True,fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Dropout,BatchNormalization,Flatten,Input,AveragePooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "baseModel=MobileNetV2(include_top=False,input_tensor=Input(shape=(224,224,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "\n",
    "mask_detect = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_detect.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(training_data, labels,test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9493\n",
      "Epoch 00001: val_loss improved from inf to 0.03693, saving model to model\\\n",
      "INFO:tensorflow:Assets written to: model\\assets\n",
      "87/87 [==============================] - 47s 536ms/step - loss: 0.1318 - accuracy: 0.9493 - val_loss: 0.0369 - val_accuracy: 0.9797\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9822\n",
      "Epoch 00002: val_loss improved from 0.03693 to 0.03163, saving model to model\\\n",
      "INFO:tensorflow:Assets written to: model\\assets\n",
      "87/87 [==============================] - 46s 531ms/step - loss: 0.0558 - accuracy: 0.9822 - val_loss: 0.0316 - val_accuracy: 0.9899\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9837\n",
      "Epoch 00003: val_loss did not improve from 0.03163\n",
      "87/87 [==============================] - 22s 251ms/step - loss: 0.0440 - accuracy: 0.9837 - val_loss: 0.0324 - val_accuracy: 0.9841\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9906\n",
      "Epoch 00004: val_loss improved from 0.03163 to 0.01689, saving model to model\\\n",
      "INFO:tensorflow:Assets written to: model\\assets\n",
      "87/87 [==============================] - 46s 526ms/step - loss: 0.0232 - accuracy: 0.9906 - val_loss: 0.0169 - val_accuracy: 0.9942\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9906\n",
      "Epoch 00005: val_loss did not improve from 0.01689\n",
      "87/87 [==============================] - 22s 253ms/step - loss: 0.0280 - accuracy: 0.9906 - val_loss: 0.0265 - val_accuracy: 0.9884\n",
      "Epoch 6/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9877\n",
      "Epoch 00006: val_loss did not improve from 0.01689\n",
      "87/87 [==============================] - 22s 254ms/step - loss: 0.0313 - accuracy: 0.9877 - val_loss: 0.0199 - val_accuracy: 0.9899\n",
      "Epoch 7/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9877\n",
      "Epoch 00007: val_loss improved from 0.01689 to 0.01565, saving model to model\\\n",
      "INFO:tensorflow:Assets written to: model\\assets\n",
      "87/87 [==============================] - 46s 533ms/step - loss: 0.0356 - accuracy: 0.9877 - val_loss: 0.0157 - val_accuracy: 0.9942\n",
      "Epoch 8/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9909\n",
      "Epoch 00008: val_loss improved from 0.01565 to 0.01099, saving model to model\\\n",
      "INFO:tensorflow:Assets written to: model\\assets\n",
      "87/87 [==============================] - 45s 521ms/step - loss: 0.0302 - accuracy: 0.9909 - val_loss: 0.0110 - val_accuracy: 0.9986\n",
      "Epoch 9/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9902\n",
      "Epoch 00009: val_loss did not improve from 0.01099\n",
      "87/87 [==============================] - 22s 255ms/step - loss: 0.0252 - accuracy: 0.9902 - val_loss: 0.0188 - val_accuracy: 0.9899\n",
      "Epoch 10/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9931\n",
      "Epoch 00010: val_loss improved from 0.01099 to 0.01072, saving model to model\\\n",
      "INFO:tensorflow:Assets written to: model\\assets\n",
      "87/87 [==============================] - 46s 525ms/step - loss: 0.0191 - accuracy: 0.9931 - val_loss: 0.0107 - val_accuracy: 0.9957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f55e344a60>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
    "checkpoint=ModelCheckpoint('model/',monitor='val_loss',mode='min',save_best_only=True,verbose=1)\n",
    "early=EarlyStopping(monitor='val_loss',min_delta=0,patience=15,verbose=1,restore_best_weights=True)\n",
    "callbacks=[checkpoint,early]\n",
    "mask_detect.fit(data_aug.flow(trainX,trainY,batch_size=32),batch_size=32,epochs=10,validation_data=(testX,testY),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "face_detect=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    detect_face=face_detect.detectMultiScale(frame,1.1,4)\n",
    "    image = frame\n",
    "    for (x,y,w,h) in detect_face:\n",
    "        faceROI=image[y:y+h+50,x:x+50+w]\n",
    "        arr=faceROI\n",
    "        arr=cv2.resize(arr,(224,224))\n",
    "        arr=arr/255.0\n",
    "        arr=arr.reshape((1,224,224,3))\n",
    "        pred=(np.argmax(mask_detect.predict(arr)))\n",
    "#         print(pred)\n",
    "        if pred==0:\n",
    "            text='with_mask'\n",
    "            cv2.putText(frame,text=text,org=(30,30),fontFace=cv2.FONT_HERSHEY_PLAIN,fontScale=1,thickness=2,color=(0,255,0))\n",
    "        else:\n",
    "            text='without_mask'\n",
    "            cv2.putText(frame,text=text,org=(30,30),fontFace=cv2.FONT_HERSHEY_PLAIN,fontScale=1,thickness=2,color=(0,0,255))\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),color=(255,0,0),thickness=1)\n",
    "        \n",
    "        cv2.imshow('mask',faceROI)\n",
    "    cv2.imshow('farme',frame)\n",
    "    if cv2.waitKey(1) & 0xFF==27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 9s 403ms/step - loss: 0.0206 - accuracy: 0.9942\n",
      "[0.0206452589482069, 0.9942113161087036]\n"
     ]
    }
   ],
   "source": [
    "print(mask_detect.evaluate(testX,testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_detect=load_model('new_mask_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-325bb2a2a4f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_detect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'testX' is not defined"
     ]
    }
   ],
   "source": [
    "print(mask_detect.evaluate(testX,testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
