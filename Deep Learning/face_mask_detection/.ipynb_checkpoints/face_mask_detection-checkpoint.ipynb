{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2,preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shiva\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\PIL\\Image.py:951: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY = r\"dataset/\"\n",
    "CATEGORIES = [\"with_mask\", \"without_mask\"]\n",
    "training_data = []\n",
    "labels = []\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY, category)\n",
    "    indexes=CATEGORIES.index(category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = load_img(img_path, target_size=(224, 224))\n",
    "        image = img_to_array(image)\n",
    "        image=preprocess_input(image)\n",
    "        training_data.append(image)\n",
    "        labels.append(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=np.array(training_data)\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(training_data, labels,test_size=0.20, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = ImageDataGenerator(rotation_range=20,zoom_range=0.15,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.15,horizontal_flip=True,fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Dropout,BatchNormalization,Flatten,Input,AveragePooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "baseModel=MobileNet(include_top=False,input_tensor=Input(shape=(224,224,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(512, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "\n",
    "mask_detect = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_detect.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(training_data, labels,test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.1666 - accuracy: 0.9449\n",
      "Epoch 00001: val_loss improved from inf to 0.02066, saving model to model\\\n",
      "INFO:tensorflow:Assets written to: model\\assets\n",
      "87/87 [==============================] - 42s 478ms/step - loss: 0.1666 - accuracy: 0.9449 - val_loss: 0.0207 - val_accuracy: 0.9928\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9877\n",
      "Epoch 00002: val_loss did not improve from 0.02066\n",
      "87/87 [==============================] - 23s 270ms/step - loss: 0.0326 - accuracy: 0.9877 - val_loss: 0.0302 - val_accuracy: 0.9884\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9862\n",
      "Epoch 00003: val_loss did not improve from 0.02066\n",
      "87/87 [==============================] - 23s 266ms/step - loss: 0.0355 - accuracy: 0.9862 - val_loss: 0.0346 - val_accuracy: 0.9899\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9859\n",
      "Epoch 00004: val_loss improved from 0.02066 to 0.01665, saving model to model\\\n",
      "INFO:tensorflow:Assets written to: model\\assets\n",
      "87/87 [==============================] - 37s 420ms/step - loss: 0.0358 - accuracy: 0.9859 - val_loss: 0.0166 - val_accuracy: 0.9942\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9931\n",
      "Epoch 00005: val_loss did not improve from 0.01665\n",
      "87/87 [==============================] - 23s 269ms/step - loss: 0.0174 - accuracy: 0.9931 - val_loss: 0.0222 - val_accuracy: 0.9928\n",
      "Epoch 6/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9884\n",
      "Epoch 00006: val_loss improved from 0.01665 to 0.01246, saving model to model\\\n",
      "INFO:tensorflow:Assets written to: model\\assets\n",
      "87/87 [==============================] - 37s 422ms/step - loss: 0.0325 - accuracy: 0.9884 - val_loss: 0.0125 - val_accuracy: 0.9971\n",
      "Epoch 7/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9902\n",
      "Epoch 00007: val_loss did not improve from 0.01246\n",
      "87/87 [==============================] - 24s 273ms/step - loss: 0.0323 - accuracy: 0.9902 - val_loss: 0.0367 - val_accuracy: 0.9928\n",
      "Epoch 8/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9924\n",
      "Epoch 00008: val_loss did not improve from 0.01246\n",
      "87/87 [==============================] - 24s 272ms/step - loss: 0.0237 - accuracy: 0.9924 - val_loss: 0.0213 - val_accuracy: 0.9928\n",
      "Epoch 9/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9946\n",
      "Epoch 00009: val_loss did not improve from 0.01246\n",
      "87/87 [==============================] - 24s 275ms/step - loss: 0.0177 - accuracy: 0.9946 - val_loss: 0.0182 - val_accuracy: 0.9957\n",
      "Epoch 10/10\n",
      "87/87 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9967\n",
      "Epoch 00010: val_loss did not improve from 0.01246\n",
      "87/87 [==============================] - 24s 275ms/step - loss: 0.0107 - accuracy: 0.9967 - val_loss: 0.0206 - val_accuracy: 0.9942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23bdb1d8580>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
    "checkpoint=ModelCheckpoint('model/',monitor='val_loss',mode='min',save_best_only=True,verbose=1)\n",
    "early=EarlyStopping(monitor='val_loss',min_delta=0,patience=15,verbose=1,restore_best_weights=True)\n",
    "callbacks=[checkpoint,early]\n",
    "mask_detect.fit(data_aug.flow(trainX,trainY,batch_size=32),batch_size=32,epochs=10,validation_data=(testX,testY),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "face_detect=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    detect_face=face_detect.detectMultiScale(frame,1.1,4)\n",
    "    image = frame\n",
    "    for (x,y,w,h) in detect_face:\n",
    "        faceROI=image[y:y+h+50,x:x+50+w]\n",
    "        arr=faceROI\n",
    "        arr=cv2.resize(arr,(224,224))\n",
    "        arr=arr/255.0\n",
    "        arr=arr.reshape((1,224,224,3))\n",
    "        pred=(np.argmax(mask_detect.predict(arr)))\n",
    "#         print(pred)\n",
    "        if pred==0:\n",
    "            text='with_mask'\n",
    "            cv2.putText(frame,text=text,org=(30,30),fontFace=cv2.FONT_HERSHEY_PLAIN,fontScale=1,thickness=2,color=(0,255,0))\n",
    "        else:\n",
    "            text='without_mask'\n",
    "            cv2.putText(frame,text=text,org=(30,30),fontFace=cv2.FONT_HERSHEY_PLAIN,fontScale=1,thickness=2,color=(0,0,255))\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),color=(255,0,0),thickness=1)\n",
    "        \n",
    "        cv2.imshow('mask',faceROI)\n",
    "    cv2.imshow('farme',frame)\n",
    "    if cv2.waitKey(1) & 0xFF==27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 9s 403ms/step - loss: 0.0206 - accuracy: 0.9942\n",
      "[0.0206452589482069, 0.9942113161087036]\n"
     ]
    }
   ],
   "source": [
    "print(mask_detect.evaluate(testX,testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_detect=load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
